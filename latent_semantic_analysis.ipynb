{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "latent_semantic_analysis.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouW_BgZFdlNh"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from textblob import TextBlob, Word, Blobber\n",
        "import pandas as pd\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSKiyQg1eBEr"
      },
      "source": [
        "data = pd.read_csv(\"/content/tweet.csv\")\n",
        "Rev_tweet=data[\"tweet\"]\n",
        "Rev_tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1syhmYmbee99"
      },
      "source": [
        "def preprocess(text):\n",
        "    clean_data = []\n",
        "    for x in (text[:]):\n",
        "        # remove HTML tags \n",
        "        new_text = re.sub('<.*?>', '', x)   \n",
        "\n",
        "        # remove punc.\n",
        "        new_text = re.sub(r'[^\\w\\s]', '', new_text)\n",
        "\n",
        "        # remove numbers \n",
        "        new_text = re.sub(r'\\d+','',new_text)\n",
        "\n",
        "        # lower case\n",
        "        new_text = new_text.lower() \n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  \n",
        "        u\"\\U0001F300-\\U0001F5FF\" \n",
        "        u\"\\U0001F680-\\U0001F6FF\"  \n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "        new_text = emoji_pattern.sub(r'', new_text)         \n",
        "        if new_text != '':\n",
        "            clean_data.append(new_text)\n",
        "    return clean_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXVvvyxbeqDv"
      },
      "source": [
        "data['clean_text']=preprocess(Rev_tweet)\n",
        "data['clean_text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHCW-xnie5aj"
      },
      "source": [
        "data['clean_tweets'] = data['clean_text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "data['clean_tweets'] = data['clean_tweets'].fillna('').apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
        "data['clean_tweets'] = data['clean_tweets'].fillna('').apply(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw5xuOavfXjY"
      },
      "source": [
        "data['clean_tweets']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsA1ktyTfZ1P"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "# If nltk stop word is not downloaded\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTfR30M9feLc"
      },
      "source": [
        "# tokenization\n",
        "tokenized_doc = data['clean_tweets'].fillna('').apply(lambda x: x.split())\n",
        "# remove stop-words\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us6yDw8XfiSj"
      },
      "source": [
        "# de-tokenization\n",
        "detokenized_doc = []\n",
        "for i in range(len(data)):\n",
        "    t = ' '.join(tokenized_doc[i])\n",
        "    detokenized_doc.append(t)\n",
        "data['clean_tweets'] = detokenized_doc\n",
        "\n",
        "data['clean_tweets']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpQRlpsPfkra"
      },
      "source": [
        "# TF-IDF vector\n",
        "vectorizer = TfidfVectorizer(stop_words='english', smooth_idf=True)\n",
        "X = vectorizer.fit_transform(data['clean_tweets'])\n",
        "X.toarray()\n",
        "\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FKsTwFyfnOh"
      },
      "source": [
        "# SVD represent documents and terms in vectors \n",
        "svd_model = TruncatedSVD(n_components=2, algorithm='randomized', n_iter=100, random_state=122)\n",
        "lsa = svd_model.fit_transform(X)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93OYK9TFf7IY"
      },
      "source": [
        "#Documents - Topic vector\n",
        "pd.options.display.float_format = '{:,.16f}'.format\n",
        "topic_encoded_df = pd.DataFrame(lsa, columns = [\"topic_1\", \"topic_2\"])\n",
        "topic_encoded_df[\"documents\"] = data['clean_tweets']\n",
        "display(topic_encoded_df[[\"documents\", \"topic_1\", \"topic_2\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey1YBxlEf9EE"
      },
      "source": [
        "# Features or words used as features \n",
        "dictionary = vectorizer.get_feature_names()\n",
        "dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvRqa3blgAXQ"
      },
      "source": [
        "# Term-Topic matrix\n",
        "encoding_matrix = pd.DataFrame(svd_model.components_, index = [\"topic_1\",\"topic_2\"], columns = (dictionary)).T\n",
        "\n",
        "encoding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd5Z8LClgJIV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}