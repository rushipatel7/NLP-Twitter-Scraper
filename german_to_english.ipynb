{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "german_to_english.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4qjf38O4pgf"
      },
      "source": [
        "### Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XioSleS24pgn"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from numpy import array, argmax, random, take\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "pd.set_option('display.max_colwidth', 200)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XLr_QRS5Tv_",
        "outputId": "2383016c-66c3-4397-9758-2d38fc94b4fa"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hosYDZKG4pgo"
      },
      "source": [
        "### Read Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB4NMgET4pgo"
      },
      "source": [
        "Our data is a text file of English-German sentence pairs. First we will read the file using the function defined below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht2s1GZD4pgo"
      },
      "source": [
        "# function to read raw text file\n",
        "def read_text(filename):\n",
        "    # open the file\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdZfUw4B4pgp"
      },
      "source": [
        "Now let's define a function to split the text into English-German pairs separated by '\\n' and then split these pairs into English sentences and German sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLx-Pa5e4pgp"
      },
      "source": [
        "# split a text into sentences\n",
        "def to_lines(text):\n",
        "    sents = text.strip().split('\\n')\n",
        "    sents = [i.split('\\t') for i in sents]\n",
        "    return sents"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyesUEdK4pgp"
      },
      "source": [
        "__Download the data from [here.](http://www.manythings.org/anki/deu-eng.zip)__ and extract \"deu.txt\" in your working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwEwTzM94pgq"
      },
      "source": [
        "data = read_text(\"gdrive/My Drive/Colab Notebooks/deu.txt\")\n",
        "deu_eng = to_lines(data)\n",
        "deu_eng = array(deu_eng)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxr7W_zI4pgq"
      },
      "source": [
        "The actual data contains over 150,000 sentence-pairs. However, we will use the first 50,000 sentence pairs only to reduce the training time of the model. You can change this number as per you system computation power."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI19wAb34pgq"
      },
      "source": [
        "deu_eng = deu_eng[:50000,:]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsf66pAc4pgq"
      },
      "source": [
        "### Text Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfI9fPTl4pgr"
      },
      "source": [
        "#### Text Cleaning\n",
        "\n",
        "Let's take a look at our data, then we will decide which pre-processing steps to adopt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcHpPLUh4pgr",
        "outputId": "8e8bd505-59b3-4209-b089-9600b63fb2d3"
      },
      "source": [
        "deu_eng"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['Go.', 'Geh.',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)'],\n",
              "       ['Hi.', 'Hallo!',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #380701 (cburgmer)'],\n",
              "       ['Hi.', 'Grüß Gott!',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #659813 (Esperantostern)'],\n",
              "       ...,\n",
              "       ['It gave me the creeps.', 'Ich bekam davon Gänsehaut.',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #42354 (CK) & #2143533 (Tamy)'],\n",
              "       ['It happened in Boston.', 'Es ist in Boston passiert.',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #6845115 (CK) & #7482756 (Luiaard)'],\n",
              "       ['It happened last year.', 'Es passierte letztes Jahr.',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #5751805 (Catriona) & #5751840 (wolfgangth)']],\n",
              "      dtype='<U537')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMWXQiBI4pgs"
      },
      "source": [
        "We will get rid of the punctuation marks, and then convert the text to lower case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KxhwIRD4pgs"
      },
      "source": [
        "# Remove punctuation\n",
        "deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]]\n",
        "deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm6Dn5wZ4pgs",
        "outputId": "c7815163-8729-46e7-ddff-9dd296e5b09e"
      },
      "source": [
        "deu_eng"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['Go', 'Geh',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)'],\n",
              "       ['Hi', 'Hallo',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #380701 (cburgmer)'],\n",
              "       ['Hi', 'Grüß Gott',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #659813 (Esperantostern)'],\n",
              "       ...,\n",
              "       ['It gave me the creeps', 'Ich bekam davon Gänsehaut',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #42354 (CK) & #2143533 (Tamy)'],\n",
              "       ['It happened in Boston', 'Es ist in Boston passiert',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #6845115 (CK) & #7482756 (Luiaard)'],\n",
              "       ['It happened last year', 'Es passierte letztes Jahr',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #5751805 (Catriona) & #5751840 (wolfgangth)']],\n",
              "      dtype='<U537')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_DAkywRc4pgt"
      },
      "source": [
        "# convert to lowercase\n",
        "for i in range(len(deu_eng)):\n",
        "    deu_eng[i,0] = deu_eng[i,0].lower()\n",
        "    \n",
        "    deu_eng[i,1] = deu_eng[i,1].lower()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arwlxUL74pgt",
        "outputId": "a3c46264-f5a4-4216-b1b5-5ad2c67582d2"
      },
      "source": [
        "deu_eng"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['go', 'geh',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)'],\n",
              "       ['hi', 'hallo',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #380701 (cburgmer)'],\n",
              "       ['hi', 'grüß gott',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #659813 (Esperantostern)'],\n",
              "       ...,\n",
              "       ['it gave me the creeps', 'ich bekam davon gänsehaut',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #42354 (CK) & #2143533 (Tamy)'],\n",
              "       ['it happened in boston', 'es ist in boston passiert',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #6845115 (CK) & #7482756 (Luiaard)'],\n",
              "       ['it happened last year', 'es passierte letztes jahr',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #5751805 (Catriona) & #5751840 (wolfgangth)']],\n",
              "      dtype='<U537')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1-7YjTe4pgt"
      },
      "source": [
        "#### Text to Sequence Conversion\n",
        "\n",
        "To feed our data in a Seq2Seq model, we will have to convert both the input and the output sentences into integer sequences of fixed length. Before that, let's visualise the length of the sentences. We will capture the lengths of all the sentences in two separate lists for English and German, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2UW8BwP4pgu"
      },
      "source": [
        "# empty lists\n",
        "eng_l = []\n",
        "deu_l = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in deu_eng[:,0]:\n",
        "    eng_l.append(len(i.split()))\n",
        "\n",
        "for i in deu_eng[:,1]:\n",
        "    deu_l.append(len(i.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR72CZny4pgu"
      },
      "source": [
        "length_df = pd.DataFrame({'eng':eng_l, 'deu':deu_l})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6onedMFM4pgu",
        "outputId": "6a905e87-87f1-420d-a31d-4070f7ff2203"
      },
      "source": [
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAdwklEQVR4nO3dfZRcdZ3n8ffHoExEWeTBHkjQ4E5k5UGjyWSyhzPYs9kZMqICrkJcRkDYQTkw4pmcsybunJWjm3PYnQnMEhYUkElww0OGB5PVoDKMvehZAgaMNhCzBNJKIEOUB0l0NpLw3T/ur8zt6urqer63uj+vc/pU1e/eW/Xt6qr+3t/v/u79KiIwMzN7XdEBmJlZOTghmJkZ4IRgZmaJE4KZmQFOCGZmljghmJkZ4IQwKUhaJem/FB2HmfU3JwQzMwOcEMzMLHFC6EOS3ivpUUm7Jd0B/E5u2QclbZb0sqT/I+nduWUh6fdyjz3UZH1B0jGS7pL0c0nbJX0mtV8haa2kW9L34XFJ83LbvU/SD9Oyv5d0hz/z43NC6DOS3gB8HfgacDjw98C/S8veB9wMfAo4AvgKsF7SwcVEa9Y+Sa8D/hfwI2AGsBD4rKTT0iofBm4HDgPWA9em7d4A3AOsIvuu3Aac1cvY+40TQv9ZALwe+NuIeDUi7gR+kJb9OfCViHgoIvZHxGpgb9rGrF/9PnBURHwxIn4TEU8DNwKL0/LvR8SGiNhPtqP0ntS+ADgIuCZ9V+4GHu518P3koKIDsKYdAzwbo69K+NN0+3bgfEl/kVv2hrSNWb96O3CMpJdzbdOA75F99v8p1/5r4HckHUTt78oz3Q62n7mH0H92AjMkKdf2tnT7DLA8Ig7L/bwxIm5Ly38NvDG33e/2IF6zdj0DbK/6XL85Ij4wwXa1vivHdi/M/ueE0H8eBPYBn5F0kKSPAPPTshuBT0v6A2UOkXS6pDen5ZuBfy9pmqRFwPt7H75Z0x4GXpH0OUnT0+f3JEm/P8F2DwL7gcvSd+UMDnxXrAYnhD4TEb8BPgJcALwEnAPcnZZtIjuOcG1ati2tV3E58CHgZeBcsoPTZqWWjg18CJgDbAd+AdwE/IsJtqt8Vy4i+8z/GfANsuNqVoNcIMfMpgpJDwFfjoi/KzqWMnIPwcwmLUnvl/S7acjofODdwLeKjqusPMvIzCaz44G1wJuAp4CPRsTOYkMqLw8ZmZkZ4CEjMzNL+nbI6Mgjj4xZs2YVHUZTfvWrX3HIIYcUHUZLJmvsjzzyyC8i4qgeh9SSIj7z/fZ3d7wTq/eZ79uEMGvWLDZt2lR0GE0ZGhpicHCw6DBaMlljl/TTmgtKqIjPfL/93R3vxOp95j1kZGZmgBOCmZklTghmZgY4IZiZWeKEYGZmgBOCmZklTghmZgY0kBAkHSvpu5K2pALWl6f2wyXdJ+nJdPuW3DbLJG2TtDVX9xRJcyUNp2XXVApXSDo4Fb/eJukhSbM6/6uamVk9jfQQ9gFLIuJdZDVKL5V0ArAUuD8iZgP3p8ekZYuBE4FFwHWSpqXnuh64GJidfhal9ouAlyLi94Crgf/agd/NzMyaMOGZyunKgDvT/d2StgAzgDOAwbTaamAI+Fxqvz0i9gLbJW0D5ksaAQ6NiAcBJN0CnAncm7a5Ij3XncC1khQlv/LerKXfHNM2cuXpBURiZgDDz/6SC6q+l/5ONq6pYwhpKOe9wEPAQOUysun2rWm1GYwuZL0jtc1I96vbR20TEfuAXwJHNBObmZm1p+FrGUl6E3AX8NmIeGV03erRq9Zoizrt9bapjuFisiEnBgYGGBoamiDq7lpy8r4xbfVi2rNnT+Ext8qxm01+DSUESa8nSwZrIuLu1Py8pKMjYqeko4FdqX0HcGxu85nAc6l9Zo32/DY7JB1EViv1xeo4IuIG4AaAefPmRdEXsarumgKMnDs47vr9duGtPMduNvk1MstIwFeBLRFxVW7ReuD8dP98YF2ufXGaOXQc2cHjh9Ow0m5JC9Jznle1TeW5Pgr8Y9mPH5iZTTaN9BBOAT4BDEvanNo+D1wJrJV0EfAz4GMAEfG4pLXAE2QzlC6NiP1pu0uAVcB0soPJ96b2rwJfSwegXySbpWRmZj3UyCyj71N7jB9g4TjbLAeW12jfBJxUo/3/kRKKmZkVw2cqm1XxyZg2VTkhmI3lkzFtSnJCMKsSETsj4tF0fzeQPxlzdVptNdmJlZA7GTMitgOVkzGPJp2MmSZJ3FK1TeW57gQWVnoPZkXp25rKZr1Q72RMSfmTMTfmNqucdPkqDZ6MKalyMuYvql6/0HNv+u0cjoHpY88PKnP8ZXt/nRDMxlGGkzGLPvem387hWLlmHSuGR/9bq3duUNHK9v56yMishnonY6blnToZk3onY5r1khOCWRWfjGlTlYeMzMbyyZg2JTkhmFXxyZg2VXnIyMzMACcEMzNLnBDMzAxwQjAzs8QJwczMACcEMzNLnBDMzAxwQjAzs6SRmso3S9ol6bFc2x2SNqefkcrZnJJmSfrn3LIv57ZxoRAzsxJrpIewigNFPQCIiHMiYk5EzCG7ANjducVPVZZFxKdz7S4UYmZWYhMmhIh4gHGuwpj28s8Gbqv3HC4UYmZWfu1ey+gPgecj4slc23GSfgi8AvxVRHyPrBhIW4VCoPhiIdWqC3FA/WIcZSuG0QzHbjb5tZsQPs7o3sFO4G0R8YKkucDXJZ1IBwqFQPHFQqpdsPSbY9rqFeMoWzGMZjh2s8mv5YSQinp8BJhbaYuIvcDedP8RSU8B76SxQiE7XCjEzKw47Uw7/bfATyLit0NBko6SNC3dfwfZweOnXSjEzKz8Gpl2ehvwIHC8pB2pOAhkBT2qDyafCvxY0o/IDhB/OiIqe/uXADcB24CnGF0o5IhUKOQvgaVt/D5mZtaiCYeMIuLj47RfUKPtLrJpqLXWd6EQ6xuSbgY+COyKiJNS2x3A8WmVw4CXI2JOOndmC7A1LdtYmXKdjqWtIquYtgG4PCJC0sFks+3mAi8A50TESPd/M7Px+Uxls9pW4fNvbIpxQjCrweff2FTkmspmzevZ+TdFn3vTb+dwDEwfe35QmeMv2/vrhGDWvJ6df1P0uTf9dg7HyjXrWDE8+t9avXODila299cJwawJPv/GJjMfQzBrjs+/sUnLCcGsBp9/Y1ORh4zMavD5NzYVuYdgZmaAE4KZmSVOCGZmBjghmJlZ4oRgZmaAE4KZmSVOCGZmBjghmJlZ4oRgZmZAYyU0b5a0S9JjubYrJD0raXP6+UBu2TJJ2yRtlXRarn2upOG07JrKtd8lHSzpjtT+UKo+ZWZmPdZID2EVVZWjkqtzFaI2AEg6gexaLyemba6rXPQLV44yMyu1CRNCvcpRNZwB3B4ReyNiO9kFvea7cpSZWfm1c3G7yySdB2wClkTES2RVoDbm1qlUiHqVNitHQfHVo6pVV2aC+tWZylYdqRmO3Tpp1tJvjno8cuXpBUViea0mhOuBL5FVePoSsAK4kPGrQLVdOQqKrx5V7YKqDzXUr85UtupIzXDsZpNfS7OMIuL5iNgfEa8BNwLz06JKFaiKSoWoRipHVapRuXKUmVkBWkoI6ZhAxVlAZQbSemBxmjl0HNnB44ddOcr6jWfX2VQ04ZBRqhw1CBwpaQfwBWBQ0hyyoZ0R4FMAEfG4pLXAE8A+4NKI2J+e6hKyGUvTyapG5StHfS1VjnqRbJaSWdFWAdeSTYDIuzoi/ibfUDW77hjgHyS9M332K7PrNgIbyGbX3Utudp2kxWSz687p3q9jNrEJE8I4laO+Wmf95cDyGu2uHGV9IyIeaGKv/bez64DtaedmvqQR0uw6AEmV2XX3pm2uSNvfCVwrSe4dW5FcQtOsOT2dXVf0zLpuzdCqnqHXqdcYmN695+6Gss2Ac0Iwa1zPZ9cVPbOuWzO0qmfo1Zud14yVa9axYnj0v7VOPXc3lG0GnK9lZNYgz66zyc4JwaxBnl1nk52HjMxq8Ow6m4qcEMxq8Ow6m4o8ZGRmZoATgpmZJU4IZmYGOCGYmVnihGBmZoATgpmZJU4IZmYGOCGYmVnihGBmZoATgpmZJRMmhHFKCf61pJ9I+rGkeyQdltpnSfrnXInBL+e2cSlBM7MSa6SHsIqs7F/efcBJEfFu4P8Cy3LLnoqIOenn07n2SinB2emn8py/LSUIXE1WStDMzHpswoQQEQ9QdZ32iPhORFTKEm1k9DXfx0iXDT40Ih5Ml/itlBKErJTg6nT/TmBhpfdgZma904ljCBdy4JK+AMdJ+qGk/y3pD1PbDBosJQhUSgmamVkPtXX5a0n/iez672tS007gbRHxgqS5wNclnUgHSgmm1yu0vmy16tqtUL9+a9nqpzbDsZtNfi0nBEnnAx8EFlYqPUXEXmBvuv+IpKeAd9JYKcEdE5USLLq+bLXqurBQv35r2eqnNsOxm01+LSUESYuAzwHvj4hf59qPAl6MiP2S3kF28PjpiHhR0m5JC4CHyEoJrkybVUoJPkiJSgnOqvXP/srTC4jEiiDpZrIdnl0RcVJq+2vgQ8BvgKeAT0bEy2lm3BZga9p8Y2VCReopryKrmLYBuDwiQtLBZMfS5gIvAOdExEhPfjmzcTQy7fQ2sn/Wx0vaIeki4FrgzcB9VdNLTwV+LOlHZAeIPx0Rlb39S4CbgG1kX6Z8KcEjUinBvwSWduZXM2vLKjy7zqaYCXsIzZQSjIi7gLvGWeZSgtY3IuKB6nNiIuI7uYcbyXq048rPrkuPK7Pr7iWbXXdFWvVO4FpJKkPv2KYu11Q2a82FwB25x8dJ+iHwCvBXEfE9mphdJ6kyu+4X+RcpeiJFtw7IV0/I6NRrDEzv3nN3Q9kmPDghmDWpl7Prip5I0a0D8tUTMupNxmjGyjXrWDE8+t9ap567G8o24cEJwawJRcyuM+sVX9zOrEG52XUfrp5dJ2laup+fXbcT2C1pQTr7/jxgXdqsMrsOSjS7zqY29xDMakiz6waBIyXtAL5ANqvoYLLZdXBgeumpwBcl7QP2M3Z23Sqyaaf3Mnp23dfS7LoXgcU9+LXM6nJCMKvBs+tsKvKQkZmZAe4hdF3+jOclJ+9jsLhQzMzqcg/BzMwAJwQzM0ucEMzMDHBCMDOzxAnBzMwAJwQzM0ucEMzMDHBCMDOzxAnBzMyAxkpo3ixpl6THcm2HS7pP0pPp9i25ZcskbZO0VdJpufa5kobTsmvS1R+RdLCkO1L7Q9VVqszMrDca6SGsYmxt2aXA/RExG7g/PUbSCWRXbTwxbXNd5bLAuLasmVmpTZgQIuIBxhbuOANYne6vJqsTW2m/PSL2RsR2YBswP19bNl3z/ZaqbSrPdSewsNJ7MDOz3mn14nYDqfgHEbFT0ltT+wyy4uMVlRqyr9JmbVnobX3Z6rqsMLY2a7PrDEwvd33XespW+7UZ/Ry7WS91+mqn49WJbbu2LPS2vmx1zVcYW5u12XWWnLyPs0tUP7UZZav92oxWYpd0M1mpzF0RcVJqOxy4A5gFjABnR8RLadkysuHP/cBnIuLbqX0uBwrkbAAuj4iQdDBZT3ku8AJwTkSMtPFrmrWt1VlGz6dhINLtrtReqRNbUakh20htWVxb1kpkFT52ZlNMqwkhXw/2fEbXiV2cZg4dR/YFeNi1Za3f+NiZTUUTDhmNU1v2SmCtpIuAn5FKAUbE45LWAk8A+4BLI2J/eirXlrV+1/NjZ708blZLt46/VB9/69RrDEzv3nN3Q9mOb02YEMapLQuwcJz1lwPLa7S7tqxNVl07dtbL42a1dOvYUfXxt+pjb61auWYdK4ZH/1vr1HN3Q9mOzflMZbPG+diZTWpOCGaN87Ezm9Q6Pe3UbFLwsTObipwQzGrwsTObijxkZGZmgBOCmZklTghmZgY4IZiZWeKEYGZmgBOCmZklTghmZgY4IZiZWeKEYGZmgM9UNjOb0Kxa1RGvPL2ASLrLPQQzMwOcEMzMLHFCMDMzoI2EIOl4SZtzP69I+qykKyQ9m2v/QG6bZZK2Sdoq6bRc+1xJw2nZNa4ta2bWey0nhIjYGhFzImIOMBf4NXBPWnx1ZVlEbACQdALZNd9PBBYB10malta/nqxu7Oz0s6jVuMzMrDWdGjJaCDwVET+ts84ZwO0RsTcitgPbgPmpFOGhEfFgqhh1C3Bmh+IyM7MGdWra6WLgttzjyySdB2wClkTES8AMYGNunR2p7dV0v7p9DEkXk/UkGBgYYGhoqEPhj7Xk5H1j2qpfr9l1BqaPXd4v9uzZ49jJhkqBO3JN7wD+M3AY8OfAz1P753O942XARcB+4DMR8e3UPpcD1dQ2AJe7jKYVqe2EIOkNwIeBZanpeuBLQKTbFcCFQK3jAlGnfWxjxA3ADQDz5s2LwcHBdkKv64Ja847PHWxrnSUn7+PsLsbcTUNDQ3Tz/e6mTsYeEVuBOQBpyPNZsqHST5INlf5Nfv2qodJjgH+Q9M5UYrMyVLqRLCEs4kCJTbOe68SQ0Z8Cj0bE8wAR8XxE7I+I14AbgflpvR3AsbntZgLPpfaZNdrNys5DpTapdGLI6OPkhoskHR0RO9PDs4DH0v31wK2SriLbU5oNPBwR+yXtlrQAeAg4D1jZgbjMuq3rQ6W9HCatpVtDhdXDrZ16jYHp3XnuRoaHW1G2odi2EoKkNwJ/DHwq1/zfJM0hG/YZqSyLiMclrQWeAPYBl6ZuM8AlHBhLvRd3m63kejVU2sth0lq6NVRYPdxaPdTaqpVr1rFiePS/tU48dyPDw60o21BsWwkhIn4NHFHV9ok66y8Hltdo3wSc1E4sZj02Zqi0skDSjcA30kMPlVrf8JnKZq0ZM1SaW1Y9VLpY0sGSjuPAUOlOYLekBelEzPOAdb0J3aw2X+3UrEkeKrXJygnBrEkeKrXJykNGZmYGOCGYmVnihGBmZoATgpmZJU4IZmYGOCGYmVnihGBmZoATgpmZJU4IZmYGOCGYmVnihGBmZoATgpmZJU4IZmYGtJkQJI1IGpa0WdKm1Ha4pPskPZlu35Jbf5mkbZK2Sjot1z43Pc82Sdek68ObmVkPdaKH8EcRMSci5qXHS4H7I2I2cH96jKQTyGrQnggsAq6TNC1tcz1Z3djZ6WdRB+IyM7MmdGPI6Axgdbq/Gjgz1357ROyNiO3ANmB+qjR1aEQ8GBEB3JLbxqx03DO2yardhBDAdyQ9Iuni1DaQygOSbt+a2mcAz+S23ZHaZqT71e1mZeaesU067VZMOyUinpP0VuA+ST+ps26tvZ+o0z72CbKkczHAwMAAQ0NDTYbbuCUn7xvTVv16za4zMH3s8n6xZ88ex17fGcBgur8aGAI+R65nDGyXVOkZj5B6xgCSKj1jl9G0wrSVECLiuXS7S9I9wHzgeUlHR8TONBy0K62+Azg2t/lM4LnUPrNGe63XuwG4AWDevHkxODjYTvh1XbD0m2PaRs4dbGudJSfv4+wuxtxNQ0NDdPP97qYuxF7pGQfwlfS5HNUzTjtJkPV2N+a2rfSAX8U9YyuZlhOCpEOA10XE7nT/T4AvAuuB84Er0+26tMl64FZJVwHHkHWRH46I/ZJ2S1oAPAScB6xsNS6zHuhZz7iXveJautW7qu5dd+o1BqZ357kbGQ1oRdl63u30EAaAe9JxsIOAWyPiW5J+AKyVdBHwM+BjABHxuKS1wBPAPuDSiNifnusSYBUwnazL7G6zlVYve8a97BXX0q2eYXXvurpn3aqVa9axYnj0v7VOPHcjowGtKFvPu+WEEBFPA++p0f4CsHCcbZYDy2u0bwJOajUWs15xz9gms3YPKptNNe4Z26TlhGDWBPeMbTLztYzMzAxwQjAzs8QJwczMACcEMzNLfFDZzMY1/OwvR83BH7ny9AKjsW5zD8HMzAAnBDMzS5wQzMwMcEIwM7PECcHMzADPMiqFWdVXfvRMDjMrgHsIZmYGOCGYmVnihGBmZoATgpmZJS0nBEnHSvqupC2SHpd0eWq/QtKzkjannw/ktlkmaZukrZJOy7XPlTScll2jVH3EzMx6p50ewj5gSUS8C1gAXCrphLTs6oiYk342AKRli4ETgUXAdZKmpfWvJyskPjv9LGojLrOu8Y6QTWYtJ4SI2BkRj6b7u4EtwIw6m5wB3B4ReyNiO7ANmJ8Kkh8aEQ9GRAC3AGe2GpdZl3lHyCatjpyHIGkW8F6yYuGnAJdJOg/YRPbleYksWWzMbbYjtb2a7le313qdi8m+QAwMDDA0NNSJ8GtacvK+MW3Vr9fsOgPTxy6v9Tzd/L1atWfPnlLG1YhOxh4RO4Gd6f5uSQ3vCAHbJVV2hEZIO0IAkio7Qq6rbIVpOyFIehNwF/DZiHhF0vXAl4BItyuAC4Fa3eGo0z62MeIG4AaAefPmxeDgYLvhj+uCqpPFAEbOHWxrnSUn7+PsGjFXP0/1c5TB0NAQ3Xy/u6lbsfdiR6iXO0G1DEwfvcPSqdfv1k5Qdbydeu5Gdv5aUbYdrbYSgqTXkyWDNRFxN0BEPJ9bfiPwjfRwB3BsbvOZwHOpfWaNdrPS6tWOUC93gmpZuWYdK4YP/Jvo1M5Kt3aCquPt1HM3svPXirLtaLUzy0jAV4EtEXFVrv3o3GpnAY+l++uBxZIOlnQc2Zjpw6kLvlvSgvSc5wHrWo3LrNvG2xGKiP0R8RpwIzA/re4dIesb7fQQTgE+AQxL2pzaPg98XNIcsr2dEeBTABHxuKS1wBNkB+YujYj9abtLgFXAdLIxVI+jWinV2xFKOzcwdkfoVklXAcdwYEdov6TdkhaQDTmdB6zs1e9hVkvLCSEivk/tbu+GOtssB5bXaN8EnNRqLGY95B0hm7R8tVOzJnhHyCazKZsQfMlpM7PRfC0jMzMDnBDMzCxxQjAzM8AJwczMEicEMzMDnBDMzCxxQjAzM8AJwczMEicEMzMDnBDMzCyZspeu6De+1IaZdZt7CGZmBriHYGZWmOFnfzm2elyBvX/3EMzMDHBCMDOzpDQJQdIiSVslbZO0tOh4zHrBn3srk1IcQ5A0DfgfwB+TFR//gaT1EfFEsZH1N89MKjd/7q1sSpEQgPnAtoh4GkDS7cAZZHVom+Z/hI3ze1Wojn3u/Xe0TlBEFB0Dkj4KLIqI/5AefwL4g4i4rGq9i4GL08Pjga09DbR9RwK/KDqIFk3W2N8eEUf1MpiKRj73JfjM99vf3fFObNzPfFl6CLWKlo/JVBFxA3BD98PpDkmbImJe0XG0wrF3xYSf+6I/8yV+72pyvO0py0HlHcCxucczgecKisWsV/y5t1IpS0L4ATBb0nGS3gAsBtYXHJNZt/lzb6VSiiGjiNgn6TLg28A04OaIeLzgsLqhb4e7cOwd1yef+1K+d3U43jaU4qCymZkVryxDRmZmVjAnBDMzA5wQekLSiKRhSZslbSo6nolIulnSLkmP5doOl3SfpCfT7VuKjHE848R+haRn0/u/WdIHioyxH0g6VtJ3JW2R9Liky4uOaSKSpkn6oaRvFB1LIyQdJulOST9J7/O/LjomJ4Te+aOImFOmOcd1rAIWVbUtBe6PiNnA/elxGa1ibOwAV6f3f05EbOhxTP1oH7AkIt4FLAAulXRCwTFN5HJgS9FBNOG/A9+KiH8FvIcSxO6EYGNExAPAi1XNZwCr0/3VwJk9DapB48RuTYqInRHxaLq/m+yf1YxioxqfpJnA6cBNRcfSCEmHAqcCXwWIiN9ExMvFRuWE0CsBfEfSI+lSBP1oICJ2QvbPAnhrwfE06zJJP05DSqUc7iorSbOA9wIPFRtJXX8L/EfgtaIDadA7gJ8Df5eGuW6SdEjRQTkh9MYpEfE+4E/Jut6nFh3QFHM98C+BOcBOYEWx4fQPSW8C7gI+GxGvFB1PLZI+COyKiEeKjqUJBwHvA66PiPcCv6IEw7BOCD0QEc+l213APWRXuew3z0s6GiDd7io4noZFxPMRsT8iXgNupD/f/56T9HqyZLAmIu4uOp46TgE+LGkEuB34N5L+Z7EhTWgHsCMiKr2uO8kSRKGcELpM0iGS3ly5D/wJ8Fj9rUppPXB+un8+sK7AWJpSSWTJWfTn+99TkkQ2vr0lIq4qOp56ImJZRMyMiFlkl//4x4j4s4LDqisi/gl4RtLxqWkhLV7uv5NKcemKSW4AuCf7fnEQcGtEfKvYkOqTdBswCBwpaQfwBeBKYK2ki4CfAR8rLsLxjRP7oKQ5ZMdyRoBPFRZg/zgF+AQwLGlzavu8Z2h11F8Aa9J1rJ4GPllwPL50hZmZZTxkZGZmgBOCmZklTghmZgY4IZiZWeKEYGZmgBOCmZklTghmZgbA/wfc1SAKm7cEUQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smO6of9_4pgu"
      },
      "source": [
        "The maximum length of the German sentences is 11 and that of the English phrases is 8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dje83tO94pgu"
      },
      "source": [
        "Let's vectorize our text data by using Keras's Tokenizer() class. It will turn our sentences into sequences of integers. Then we will pad those sequences with zeros to make all the sequences of same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIcTqtA94pgv"
      },
      "source": [
        "# function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvTMD5174pgv",
        "outputId": "cd08e562-adc5-491d-e03c-faad15fe796e"
      },
      "source": [
        "# prepare english tokenizer\n",
        "eng_tokenizer = tokenization(deu_eng[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "\n",
        "eng_length = 8\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 6238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y_EnniB4pgv",
        "outputId": "81a13937-3b55-46db-b23c-04593fda39dc"
      },
      "source": [
        "# prepare Deutch tokenizer\n",
        "deu_tokenizer = tokenization(deu_eng[:, 1])\n",
        "deu_vocab_size = len(deu_tokenizer.word_index) + 1\n",
        "\n",
        "deu_length = 8\n",
        "print('Deutch Vocabulary Size: %d' % deu_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deutch Vocabulary Size: 10336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjKpWiUG4pgw"
      },
      "source": [
        "Given below is a function to prepare the sequences. It will also perform sequence padding to a maximum sentence length as mentioned above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIZHZvmU4pgw"
      },
      "source": [
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    # integer encode sequences\n",
        "    seq = tokenizer.texts_to_sequences(lines)\n",
        "    # pad sequences with 0 values\n",
        "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "    return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HevIkJm74pgw"
      },
      "source": [
        "### Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jBy7Le04pgw"
      },
      "source": [
        "We will now split the data into train and test set for model training and evaluation, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_50n66qo4pgw"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(deu_eng, test_size=0.2, random_state = 12)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMsdRS9cJ7gT"
      },
      "source": [
        "# encode and pad sequences\r\n",
        "def encode_sequences(tokenizer, length, lines):\r\n",
        "         # integer encode sequences\r\n",
        "         seq = tokenizer.texts_to_sequences(lines)\r\n",
        "         # pad sequences with 0 values\r\n",
        "         seq = pad_sequences(seq, maxlen=length, padding='post')\r\n",
        "         return seq"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc-UsiMkKjNi"
      },
      "source": [
        "def tokenization(lines):\r\n",
        "      tokenizer = Tokenizer()\r\n",
        "      tokenizer.fit_on_texts(lines)\r\n",
        "      return tokenizer"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4qd6MzeKacC",
        "outputId": "929b6ae2-8b91-4b9f-e2a4-7094a86e22f7"
      },
      "source": [
        "deu_tokenizer = tokenization(deu_eng[:, 1])\r\n",
        "deu_vocab_size = len(deu_tokenizer.word_index) + 1\r\n",
        "\r\n",
        "deu_length = 8\r\n",
        "print('Deutch Vocabulary Size: %d' % deu_vocab_size)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deutch Vocabulary Size: 10326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osGk7_XzLC7A",
        "outputId": "5befb7ac-261b-4dde-832d-3ac691b2270f"
      },
      "source": [
        "eng_tokenizer = tokenization(deu_eng[:, 0])\r\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\r\n",
        "\r\n",
        "eng_length = 8\r\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 6241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOw81KCw4pgx"
      },
      "source": [
        "It's time to encode the sentences. We will encode German sentences as the input sequences and English sentences as the target sequences. It will be done for both train and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZGRNndP4pgx"
      },
      "source": [
        "# prepare training data\n",
        "trainX = encode_sequences(deu_tokenizer, deu_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5StmdxP4pgx"
      },
      "source": [
        "# prepare validation data\n",
        "testX = encode_sequences(deu_tokenizer, deu_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-LdHQSQ4pgx"
      },
      "source": [
        "Now comes the exciting part! Let us define our Seq2Seq model architecture. We are using an Embedding layer and an LSTM layer as our encoder and another LSTM layer followed by a Dense layer as the decoder.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGqXMoXH4pgx"
      },
      "source": [
        "# build NMT model\n",
        "def build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
        "    model.add(LSTM(units))\n",
        "    model.add(RepeatVector(out_timesteps))\n",
        "    model.add(LSTM(units, return_sequences=True))\n",
        "    model.add(Dense(out_vocab, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKIAVOHh4pgy"
      },
      "source": [
        "We are using RMSprop optimizer in this model as it is usually a good choice for recurrent neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTU-_rid4pgy"
      },
      "source": [
        "model = build_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)\n",
        "rms = optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM31iYVo4pgz"
      },
      "source": [
        "Please note that we have used __'sparse_categorical_crossentropy'__ as the loss function because it allows us to use the target sequence as it is instead of one hot encoded format. One hot encoding the target sequences with such a huge vocabulary might consume our system's entire memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsTlEceb4pgz"
      },
      "source": [
        "It seems we are all set to start training our model. We will train it for 30 epochs and with a batch size of 512. You may change and play these hyperparameters. We will also be using __ModelCheckpoint()__ to save the best model with lowest validation loss. I personally prefer this method over early stopping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRIsbrog4pgz",
        "outputId": "1b6c4a71-adf9-418d-ba51-b8af2e00aa9c"
      },
      "source": [
        "filename = 'model.h1.24_jan_19'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), \n",
        "          epochs=30, batch_size=512, \n",
        "          validation_split = 0.2,\n",
        "          callbacks=[checkpoint], verbose=1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 3.4143\n",
            "Epoch 00001: val_loss improved from inf to 2.84308, saving model to model.h1.24_jan_19\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 22s 351ms/step - loss: 3.4143 - val_loss: 2.8431\n",
            "Epoch 2/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.7669\n",
            "Epoch 00002: val_loss improved from 2.84308 to 2.71658, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 331ms/step - loss: 2.7669 - val_loss: 2.7166\n",
            "Epoch 3/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.5883\n",
            "Epoch 00003: val_loss improved from 2.71658 to 2.54598, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 339ms/step - loss: 2.5883 - val_loss: 2.5460\n",
            "Epoch 4/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.4087\n",
            "Epoch 00004: val_loss improved from 2.54598 to 2.40366, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 333ms/step - loss: 2.4087 - val_loss: 2.4037\n",
            "Epoch 5/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.2706\n",
            "Epoch 00005: val_loss improved from 2.40366 to 2.30666, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 333ms/step - loss: 2.2706 - val_loss: 2.3067\n",
            "Epoch 6/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.1400\n",
            "Epoch 00006: val_loss improved from 2.30666 to 2.19678, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 334ms/step - loss: 2.1400 - val_loss: 2.1968\n",
            "Epoch 7/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.0192\n",
            "Epoch 00007: val_loss improved from 2.19678 to 2.10961, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 338ms/step - loss: 2.0192 - val_loss: 2.1096\n",
            "Epoch 8/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.9060\n",
            "Epoch 00008: val_loss improved from 2.10961 to 2.03095, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 332ms/step - loss: 1.9060 - val_loss: 2.0309\n",
            "Epoch 9/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.8018\n",
            "Epoch 00009: val_loss improved from 2.03095 to 1.95829, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 22s 344ms/step - loss: 1.8018 - val_loss: 1.9583\n",
            "Epoch 10/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.7012\n",
            "Epoch 00010: val_loss improved from 1.95829 to 1.88722, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 335ms/step - loss: 1.7012 - val_loss: 1.8872\n",
            "Epoch 11/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.6056\n",
            "Epoch 00011: val_loss improved from 1.88722 to 1.85964, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 334ms/step - loss: 1.6056 - val_loss: 1.8596\n",
            "Epoch 12/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.5113\n",
            "Epoch 00012: val_loss improved from 1.85964 to 1.76802, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 337ms/step - loss: 1.5113 - val_loss: 1.7680\n",
            "Epoch 13/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.4197\n",
            "Epoch 00013: val_loss improved from 1.76802 to 1.71979, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 338ms/step - loss: 1.4197 - val_loss: 1.7198\n",
            "Epoch 14/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.3335\n",
            "Epoch 00014: val_loss improved from 1.71979 to 1.66377, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 335ms/step - loss: 1.3335 - val_loss: 1.6638\n",
            "Epoch 15/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.2529\n",
            "Epoch 00015: val_loss improved from 1.66377 to 1.61098, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 331ms/step - loss: 1.2529 - val_loss: 1.6110\n",
            "Epoch 16/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.1737\n",
            "Epoch 00016: val_loss improved from 1.61098 to 1.57971, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 337ms/step - loss: 1.1737 - val_loss: 1.5797\n",
            "Epoch 17/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.1012\n",
            "Epoch 00017: val_loss improved from 1.57971 to 1.54253, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 334ms/step - loss: 1.1012 - val_loss: 1.5425\n",
            "Epoch 18/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.0270\n",
            "Epoch 00018: val_loss improved from 1.54253 to 1.50681, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 337ms/step - loss: 1.0270 - val_loss: 1.5068\n",
            "Epoch 19/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.9587\n",
            "Epoch 00019: val_loss improved from 1.50681 to 1.47648, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 332ms/step - loss: 0.9587 - val_loss: 1.4765\n",
            "Epoch 20/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8917\n",
            "Epoch 00020: val_loss improved from 1.47648 to 1.44974, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 339ms/step - loss: 0.8917 - val_loss: 1.4497\n",
            "Epoch 21/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8287\n",
            "Epoch 00021: val_loss improved from 1.44974 to 1.44655, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 332ms/step - loss: 0.8287 - val_loss: 1.4465\n",
            "Epoch 22/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7716\n",
            "Epoch 00022: val_loss improved from 1.44655 to 1.40522, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 331ms/step - loss: 0.7716 - val_loss: 1.4052\n",
            "Epoch 23/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7157\n",
            "Epoch 00023: val_loss improved from 1.40522 to 1.38340, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 335ms/step - loss: 0.7157 - val_loss: 1.3834\n",
            "Epoch 24/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6622\n",
            "Epoch 00024: val_loss improved from 1.38340 to 1.36452, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 332ms/step - loss: 0.6622 - val_loss: 1.3645\n",
            "Epoch 25/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6121\n",
            "Epoch 00025: val_loss improved from 1.36452 to 1.33951, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 328ms/step - loss: 0.6121 - val_loss: 1.3395\n",
            "Epoch 26/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5636\n",
            "Epoch 00026: val_loss improved from 1.33951 to 1.33005, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 329ms/step - loss: 0.5636 - val_loss: 1.3301\n",
            "Epoch 27/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5192\n",
            "Epoch 00027: val_loss improved from 1.33005 to 1.31818, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 330ms/step - loss: 0.5192 - val_loss: 1.3182\n",
            "Epoch 28/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4804\n",
            "Epoch 00028: val_loss improved from 1.31818 to 1.30959, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 335ms/step - loss: 0.4804 - val_loss: 1.3096\n",
            "Epoch 29/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4405\n",
            "Epoch 00029: val_loss did not improve from 1.30959\n",
            "63/63 [==============================] - 10s 157ms/step - loss: 0.4405 - val_loss: 1.3220\n",
            "Epoch 30/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4042\n",
            "Epoch 00030: val_loss improved from 1.30959 to 1.30117, saving model to model.h1.24_jan_19\n",
            "INFO:tensorflow:Assets written to: model.h1.24_jan_19/assets\n",
            "63/63 [==============================] - 21s 331ms/step - loss: 0.4042 - val_loss: 1.3012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMtvQWLm4pgz"
      },
      "source": [
        "Let's compare the training loss and the validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "lmPUvp3b4pg0",
        "outputId": "0918b4ce-88c3-4b06-d12e-3f740cad687e"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ49urM84pg0"
      },
      "source": [
        "### Make Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTM4tZmY4pg0"
      },
      "source": [
        "Let's load the saved model to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh6oKGEO4pg1",
        "outputId": "db487eee-51c6-4649-e26a-35517b54ae3d"
      },
      "source": [
        "model = load_model('model.h1.24_jan_19')\n",
        "preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-24-aaf5ce307e3e>:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_W3ny--4pg1"
      },
      "source": [
        "def get_word(n, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == n:\n",
        "            return word\n",
        "    return None"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MvtY8WF4pg1"
      },
      "source": [
        "# convert predictions into text (English)\n",
        "preds_text = []\n",
        "for i in preds:\n",
        "    temp = []\n",
        "    for j in range(len(i)):\n",
        "        t = get_word(i[j], eng_tokenizer)\n",
        "        if j > 0:\n",
        "            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
        "                temp.append('')\n",
        "            else:\n",
        "                temp.append(t)\n",
        "             \n",
        "        else:\n",
        "            if(t == None):\n",
        "                temp.append('')\n",
        "            else:\n",
        "                temp.append(t)            \n",
        "        \n",
        "    preds_text.append(' '.join(temp))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cStKASKN4pg1"
      },
      "source": [
        "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYVGUiK74pg2"
      },
      "source": [
        "pd.set_option('display.max_colwidth', 200)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "lcpYRkDp4pg2",
        "outputId": "bc74d550-cdb9-4936-dc39-87ab3bb7797b"
      },
      "source": [
        "pred_df.head(15)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i said good morning</td>\n",
              "      <td>i slept a night</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>write tom</td>\n",
              "      <td>hug tom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dogs are smart</td>\n",
              "      <td>dogs are</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>was tom right</td>\n",
              "      <td>was tom was</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>they are my brothers</td>\n",
              "      <td>these are my</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>i am telling a story</td>\n",
              "      <td>i see a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>well give it a shot</td>\n",
              "      <td>well try to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ill try to be early</td>\n",
              "      <td>ill buy some a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>schools not fun</td>\n",
              "      <td>this isnt from</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>its not always easy</td>\n",
              "      <td>its not</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>its too dark</td>\n",
              "      <td>it is dark</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>it happens</td>\n",
              "      <td>that</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>i do think so</td>\n",
              "      <td>i said</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>hes innocent</td>\n",
              "      <td>hes innocent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>write that down</td>\n",
              "      <td>open this</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  actual            predicted\n",
              "0    i said good morning  i slept a night    \n",
              "1              write tom        hug tom      \n",
              "2         dogs are smart       dogs are      \n",
              "3          was tom right     was tom was     \n",
              "4   they are my brothers    these are my     \n",
              "5   i am telling a story         i see a     \n",
              "6    well give it a shot     well try to     \n",
              "7    ill try to be early   ill buy some a    \n",
              "8        schools not fun  this isnt from     \n",
              "9    its not always easy        its not      \n",
              "10          its too dark      it is dark     \n",
              "11            it happens          that       \n",
              "12         i do think so         i said      \n",
              "13          hes innocent   hes innocent      \n",
              "14       write that down      open this      "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "TbgsFAFU4pg2",
        "outputId": "adb00df0-d4f0-4d63-e9cf-b65f21194203"
      },
      "source": [
        "pred_df.tail(15)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9985</th>\n",
              "      <td>this is all on me</td>\n",
              "      <td>i crossed the bill</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9986</th>\n",
              "      <td>did you ever try</td>\n",
              "      <td>did you ever it yet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9987</th>\n",
              "      <td>i know tom is fast</td>\n",
              "      <td>i know tom is a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9988</th>\n",
              "      <td>where did i put it</td>\n",
              "      <td>where did i put it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9989</th>\n",
              "      <td>i was helping tom</td>\n",
              "      <td>i helped tom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9990</th>\n",
              "      <td>tom seems excited</td>\n",
              "      <td>tom seems busy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9991</th>\n",
              "      <td>i wont tell you</td>\n",
              "      <td>i wont tell you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9992</th>\n",
              "      <td>whos sick</td>\n",
              "      <td>who rich</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9993</th>\n",
              "      <td>the auction is over</td>\n",
              "      <td>the partys over</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9994</th>\n",
              "      <td>now we know why</td>\n",
              "      <td>we  know why</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>my mailbox is full</td>\n",
              "      <td>my inbox is full</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>mary is a waitress</td>\n",
              "      <td>mary is an</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>i need more room</td>\n",
              "      <td>i need more</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>he seems quite happy</td>\n",
              "      <td>he seems happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>tom is unprepared</td>\n",
              "      <td>tom is</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    actual               predicted\n",
              "9985     this is all on me  i crossed the bill    \n",
              "9986      did you ever try  did you ever it yet   \n",
              "9987    i know tom is fast      i know tom is a   \n",
              "9988    where did i put it   where did i put it   \n",
              "9989     i was helping tom       i helped tom     \n",
              "9990     tom seems excited     tom seems busy     \n",
              "9991       i wont tell you     i wont tell you    \n",
              "9992             whos sick          who rich      \n",
              "9993   the auction is over    the partys over     \n",
              "9994       now we know why        we  know why    \n",
              "9995    my mailbox is full    my inbox is full    \n",
              "9996    mary is a waitress         mary is an     \n",
              "9997      i need more room        i need more     \n",
              "9998  he seems quite happy     he seems happy     \n",
              "9999     tom is unprepared            tom is      "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "xEYwdT1X4pg2",
        "outputId": "bdefcc35-9754-458a-f2b6-d3ac1403e17b"
      },
      "source": [
        "pred_df.tail(15)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9985</th>\n",
              "      <td>this is all on me</td>\n",
              "      <td>i crossed the bill</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9986</th>\n",
              "      <td>did you ever try</td>\n",
              "      <td>did you ever it yet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9987</th>\n",
              "      <td>i know tom is fast</td>\n",
              "      <td>i know tom is a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9988</th>\n",
              "      <td>where did i put it</td>\n",
              "      <td>where did i put it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9989</th>\n",
              "      <td>i was helping tom</td>\n",
              "      <td>i helped tom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9990</th>\n",
              "      <td>tom seems excited</td>\n",
              "      <td>tom seems busy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9991</th>\n",
              "      <td>i wont tell you</td>\n",
              "      <td>i wont tell you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9992</th>\n",
              "      <td>whos sick</td>\n",
              "      <td>who rich</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9993</th>\n",
              "      <td>the auction is over</td>\n",
              "      <td>the partys over</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9994</th>\n",
              "      <td>now we know why</td>\n",
              "      <td>we  know why</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>my mailbox is full</td>\n",
              "      <td>my inbox is full</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>mary is a waitress</td>\n",
              "      <td>mary is an</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>i need more room</td>\n",
              "      <td>i need more</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>he seems quite happy</td>\n",
              "      <td>he seems happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>tom is unprepared</td>\n",
              "      <td>tom is</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    actual               predicted\n",
              "9985     this is all on me  i crossed the bill    \n",
              "9986      did you ever try  did you ever it yet   \n",
              "9987    i know tom is fast      i know tom is a   \n",
              "9988    where did i put it   where did i put it   \n",
              "9989     i was helping tom       i helped tom     \n",
              "9990     tom seems excited     tom seems busy     \n",
              "9991       i wont tell you     i wont tell you    \n",
              "9992             whos sick          who rich      \n",
              "9993   the auction is over    the partys over     \n",
              "9994       now we know why        we  know why    \n",
              "9995    my mailbox is full    my inbox is full    \n",
              "9996    mary is a waitress         mary is an     \n",
              "9997      i need more room        i need more     \n",
              "9998  he seems quite happy     he seems happy     \n",
              "9999     tom is unprepared            tom is      "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "5w-irVC04pg3",
        "outputId": "bb3e4785-863c-485b-a09b-b446b2666d75"
      },
      "source": [
        "pred_df.sample(15)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6449</th>\n",
              "      <td>i feel very good</td>\n",
              "      <td>i feel great</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2290</th>\n",
              "      <td>i forget her name</td>\n",
              "      <td>im not</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4614</th>\n",
              "      <td>that is intriguing</td>\n",
              "      <td>that is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3099</th>\n",
              "      <td>you must not smoke</td>\n",
              "      <td>you must not smoke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1083</th>\n",
              "      <td>tom has lung cancer</td>\n",
              "      <td>tom has mute</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8839</th>\n",
              "      <td>dont be a baby</td>\n",
              "      <td>dont pick your legs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5081</th>\n",
              "      <td>i wont tell tom</td>\n",
              "      <td>i wont tell tom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4236</th>\n",
              "      <td>people love freedom</td>\n",
              "      <td>this was got sense</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2220</th>\n",
              "      <td>i know tom is deaf</td>\n",
              "      <td>i know tom is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8458</th>\n",
              "      <td>is this your letter</td>\n",
              "      <td>is this your yours</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>i know you have it</td>\n",
              "      <td>i know you like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5125</th>\n",
              "      <td>i am a good boy</td>\n",
              "      <td>im a good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9962</th>\n",
              "      <td>tom is scolding mary</td>\n",
              "      <td>tom is extremely</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8237</th>\n",
              "      <td>when did you buy it</td>\n",
              "      <td>when did you buy it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7436</th>\n",
              "      <td>wait dont shoot</td>\n",
              "      <td>please dont shoot</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    actual                predicted\n",
              "6449      i feel very good        i feel great     \n",
              "2290     i forget her name             im not      \n",
              "4614    that is intriguing            that is      \n",
              "3099    you must not smoke   you must not smoke    \n",
              "1083   tom has lung cancer        tom has mute     \n",
              "8839        dont be a baby  dont pick your legs    \n",
              "5081       i wont tell tom      i wont tell tom    \n",
              "4236   people love freedom   this was got sense    \n",
              "2220    i know tom is deaf        i know tom is    \n",
              "8458   is this your letter   is this your yours    \n",
              "70      i know you have it    i know you like it   \n",
              "5125       i am a good boy           im a good     \n",
              "9962  tom is scolding mary    tom is extremely     \n",
              "8237   when did you buy it   when did you buy it   \n",
              "7436       wait dont shoot   please dont shoot     "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    }
  ]
}